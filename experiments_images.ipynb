{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "e2_9JdnZ_hwX",
    "outputId": "8bdfce79-0f3a-408f-dbfd-92b84f18a98b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "8HXBcZv0ddA_",
    "outputId": "c07d75aa-feee-4374-b993-7f21f654e065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/privacy\n",
      "  Cloning https://github.com/tensorflow/privacy to c:\\users\\hesl\\appdata\\local\\temp\\pip-req-build-vd0j9kqm\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\hesl\\anaconda3\\lib\\site-packages (from tensorflow-privacy==0.2.2) (1.3.1)\n",
      "Requirement already satisfied: mpmath in c:\\users\\hesl\\anaconda3\\lib\\site-packages (from tensorflow-privacy==0.2.2) (1.1.0)\n",
      "Collecting dm-tree~=0.1.1 (from tensorflow-privacy==0.2.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/d7/43a33fc1c9ea5e0d2d545ccd5f661ad0fabe3249399e812f7a501947a38b/dm_tree-0.1.1-cp37-cp37m-win_amd64.whl (84kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hesl\\anaconda3\\lib\\site-packages (from dm-tree~=0.1.1->tensorflow-privacy==0.2.2) (1.12.0)\n",
      "Building wheels for collected packages: tensorflow-privacy\n",
      "  Building wheel for tensorflow-privacy (setup.py): started\n",
      "  Building wheel for tensorflow-privacy (setup.py): finished with status 'done'\n",
      "  Created wheel for tensorflow-privacy: filename=tensorflow_privacy-0.2.2-cp37-none-any.whl size=83709 sha256=9b184d8cbe6dc9a9665cf8e2705fc0b72616cd49489f13beabf2990a6619d1c2\n",
      "  Stored in directory: C:\\Users\\hesl\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7n5e16qa\\wheels\\8a\\e4\\14\\41d16468ac11ec804bd21cfb75fc2e24f96b9e4c5af778f576\n",
      "Successfully built tensorflow-privacy\n",
      "Installing collected packages: dm-tree, tensorflow-privacy\n",
      "Successfully installed dm-tree-0.1.1 tensorflow-privacy-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/tensorflow/privacy 'C:\\Users\\hesl\\AppData\\Local\\Temp\\pip-req-build-vd0j9kqm'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "YqwJ4pdwJr8p",
    "outputId": "cca50759-a2ad-4e7a-d8a5-730a313c8c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOE5H_ZPNUaN"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahOTmmFiY1Zm"
   },
   "outputs": [],
   "source": [
    "def create_model2(input_shape, num_category):\n",
    "  ##model building\n",
    "  model = Sequential()\n",
    "  #convolutional layer with rectified linear unit activation\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                  activation='relu',\n",
    "                  input_shape=input_shape))\n",
    "  #32 convolution filters used each of size 3x3\n",
    "  #again\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  #64 convolution filters used each of size 3x3\n",
    "  #choose the best features via pooling\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  #randomly turn neurons on and off to improve convergence\n",
    "  model.add(Dropout(0.25))\n",
    "  #flatten since too many dimensions, we only want a classification output\n",
    "  model.add(Flatten())\n",
    "  #fully connected to get all relevant data\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  #one more dropout for convergence' sake :) \n",
    "  model.add(Dropout(0.5))\n",
    "  #output a softmax to squash the matrix into output probabilities\n",
    "  model.add(Dense(num_category, activation='softmax'))\n",
    "  #Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "  #categorical ce since we have multiple classes (10) \n",
    "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer, DPAdamGaussianOptimizer, DPAdamOptimizer\n",
    "def create_dp_model2(input_shape, num_category):\n",
    "  ##model building\n",
    "  dpopt = DPAdamGaussianOptimizer(l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=128, learning_rate=0.001)\n",
    "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "  model = Sequential()\n",
    "  #convolutional layer with rectified linear unit activation\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                  activation='relu',\n",
    "                  input_shape=input_shape))\n",
    "  #32 convolution filters used each of size 3x3\n",
    "  #again\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  #64 convolution filters used each of size 3x3\n",
    "  #choose the best features via pooling\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  #randomly turn neurons on and off to improve convergence\n",
    "  model.add(Dropout(0.25))\n",
    "  #flatten since too many dimensions, we only want a classification output\n",
    "  model.add(Flatten())\n",
    "  #fully connected to get all relevant data\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  #one more dropout for convergence' sake :) \n",
    "  model.add(Dropout(0.5))\n",
    "  #output a softmax to squash the matrix into output probabilities\n",
    "  model.add(Dense(num_category, activation='softmax'))\n",
    "  #Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "  #categorical ce since we have multiple classes (10) \n",
    "  model.compile(loss=loss,\n",
    "                optimizer=dpopt,\n",
    "                metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQjXVgNWJvTo"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model(shape, num_classes):\n",
    "\t# create model\n",
    "  model = keras.Sequential([\n",
    "      keras.layers.Conv2D(16, 8,\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            input_shape=shape),\n",
    "      keras.layers.MaxPool2D(2, 1),\n",
    "      keras.layers.Conv2D(32, 4,\n",
    "                            strides=2,\n",
    "                            padding='valid',\n",
    "                            activation='relu'),\n",
    "      keras.layers.MaxPool2D(2, 1),\n",
    "      keras.layers.Flatten(),\n",
    "      keras.layers.Dense(32, activation='relu'),\n",
    "      keras.layers.Dense(num_classes)\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "  return model\n",
    "\t# build the model\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer, DPAdamGaussianOptimizer, DPAdamOptimizer\n",
    "\n",
    "\n",
    "def create_dp_model(shape, num_classes):\n",
    "  \n",
    "  #dpopt = DPGradientDescentGaussianOptimizer(l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=250, learning_rate=0.15)\n",
    "  dpopt = DPAdamGaussianOptimizer(l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=250, learning_rate=0.001)\n",
    "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, 8,\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            input_shape=shape),\n",
    "      tf.keras.layers.MaxPool2D(2, 1),\n",
    "      tf.keras.layers.Conv2D(32, 4,\n",
    "                            strides=2,\n",
    "                            padding='valid',\n",
    "                            activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(2, 1),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(num_classes)\n",
    "  ])\n",
    "  model.compile(loss=loss,\n",
    "              optimizer=dpopt,\n",
    "              metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CdZXaAzrKK2E",
    "outputId": "655229cf-3887-4532-8a53-3d98da946169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 125s 2ms/step - loss: 3.3256 - acc: 0.7378 - val_loss: 0.0946 - val_acc: 0.9730\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.1516 - acc: 0.9569 - val_loss: 0.0651 - val_acc: 0.9790\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.1024 - acc: 0.9707 - val_loss: 0.0570 - val_acc: 0.9833\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0857 - acc: 0.9755 - val_loss: 0.0512 - val_acc: 0.9845\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0691 - acc: 0.9805 - val_loss: 0.0442 - val_acc: 0.9866\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0640 - acc: 0.9819 - val_loss: 0.0477 - val_acc: 0.9873\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0557 - acc: 0.9839 - val_loss: 0.0449 - val_acc: 0.9884\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0513 - acc: 0.9847 - val_loss: 0.0493 - val_acc: 0.9874\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0435 - acc: 0.9871 - val_loss: 0.0445 - val_acc: 0.9884\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0418 - acc: 0.9879 - val_loss: 0.0387 - val_acc: 0.9896\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0382 - acc: 0.9891 - val_loss: 0.0449 - val_acc: 0.9893\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0349 - acc: 0.9900 - val_loss: 0.0441 - val_acc: 0.9896\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0469 - val_acc: 0.9898\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0303 - acc: 0.9909 - val_loss: 0.0420 - val_acc: 0.9894\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0303 - acc: 0.9913 - val_loss: 0.0441 - val_acc: 0.9897\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0266 - acc: 0.9919 - val_loss: 0.0460 - val_acc: 0.9903\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0258 - acc: 0.9922 - val_loss: 0.0335 - val_acc: 0.9922\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0248 - acc: 0.9926 - val_loss: 0.0433 - val_acc: 0.9908\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.0387 - val_acc: 0.9907\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0372 - val_acc: 0.9915\n",
      "[0.03717723430942378, 0.9915]\n",
      "Baseline Error: 0.85%\n"
     ]
    }
   ],
   "source": [
    "model = create_model2(X_test.shape[1:], 10)\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=250, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "61iwh37WNgp_",
    "outputId": "569e7698-b96a-4e63-d171-d709210c787f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "10000/10000 [==============================] - 2s 213us/step\n",
      "[0.08146530080835644, 0.979200005531311]\n",
      "Training Model: 1\n",
      "10000/10000 [==============================] - 2s 212us/step\n",
      "[0.07629589683762954, 0.9807999730110168]\n",
      "Training Model: 2\n",
      "10000/10000 [==============================] - 2s 246us/step\n",
      "[0.06830544563137977, 0.9807999730110168]\n",
      "Training Model: 3\n",
      "10000/10000 [==============================] - 3s 251us/step\n",
      "[0.058769419142500556, 0.9840999841690063]\n",
      "10000/10000 [==============================] - 2s 217us/step\n",
      "0 : [2.261798006439209, 0.2973000109195709]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 13s 884us/step - loss: 0.3750 - accuracy: 0.8839 - val_loss: 0.0780 - val_accuracy: 0.9772\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 13s 898us/step - loss: 0.3788 - accuracy: 0.8867 - val_loss: 0.0781 - val_accuracy: 0.9755\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 933us/step - loss: 0.3866 - accuracy: 0.8810 - val_loss: 0.1029 - val_accuracy: 0.9668\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 13s 889us/step - loss: 0.3826 - accuracy: 0.8848 - val_loss: 0.0923 - val_accuracy: 0.9707\n",
      "10000/10000 [==============================] - ETA:  - 2s 225us/step\n",
      "1 : [0.07145707705430686, 0.9778000116348267]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 900us/step - loss: 0.1314 - accuracy: 0.9600 - val_loss: 0.0638 - val_accuracy: 0.9796\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 927us/step - loss: 0.1312 - accuracy: 0.9605 - val_loss: 0.0658 - val_accuracy: 0.9790\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 932us/step - loss: 0.1459 - accuracy: 0.9573 - val_loss: 0.0633 - val_accuracy: 0.9795\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 912us/step - loss: 0.1282 - accuracy: 0.9608 - val_loss: 0.0650 - val_accuracy: 0.9780\n",
      "10000/10000 [==============================] - 2s 229us/step\n",
      "2 : [0.05248534488331061, 0.9828000068664551]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 929us/step - loss: 0.0970 - accuracy: 0.9693 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 904us/step - loss: 0.0933 - accuracy: 0.9724 - val_loss: 0.0612 - val_accuracy: 0.9807\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 904us/step - loss: 0.1070 - accuracy: 0.9681 - val_loss: 0.0617 - val_accuracy: 0.9802\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 903us/step - loss: 0.0977 - accuracy: 0.9711 - val_loss: 0.0528 - val_accuracy: 0.9833\n",
      "10000/10000 [==============================] - 2s 231us/step\n",
      "3 : [0.04766450797340367, 0.9850000143051147]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 902us/step - loss: 0.0825 - accuracy: 0.9755 - val_loss: 0.0508 - val_accuracy: 0.9839\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 902us/step - loss: 0.0774 - accuracy: 0.9775 - val_loss: 0.0481 - val_accuracy: 0.9829\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 935us/step - loss: 0.0884 - accuracy: 0.9739 - val_loss: 0.0492 - val_accuracy: 0.9836\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 924us/step - loss: 0.0766 - accuracy: 0.9772 - val_loss: 0.0479 - val_accuracy: 0.9854\n",
      "10000/10000 [==============================] - 2s 238us/step\n",
      "4 : [0.03793354971548833, 0.9868000149726868]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 911us/step - loss: 0.0682 - accuracy: 0.9783 - val_loss: 0.0440 - val_accuracy: 0.9852\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 937us/step - loss: 0.0709 - accuracy: 0.9789 - val_loss: 0.0500 - val_accuracy: 0.9843\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 15s 986us/step - loss: 0.0828 - accuracy: 0.9754 - val_loss: 0.0432 - val_accuracy: 0.9852\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 950us/step - loss: 0.0680 - accuracy: 0.9795 - val_loss: 0.0446 - val_accuracy: 0.9854\n",
      "10000/10000 [==============================] - 2s 243us/step\n",
      "5 : [0.03638950860443292, 0.9865999817848206]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0657 - accuracy: 0.9790 - val_loss: 0.0467 - val_accuracy: 0.9851\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 952us/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.0452 - val_accuracy: 0.9848\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 939us/step - loss: 0.0738 - accuracy: 0.9775 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 950us/step - loss: 0.0600 - accuracy: 0.9815 - val_loss: 0.0408 - val_accuracy: 0.9870\n",
      "10000/10000 [==============================] - 2s 242us/step\n",
      "6 : [0.036176817950181434, 0.9871000051498413]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 942us/step - loss: 0.0543 - accuracy: 0.9820 - val_loss: 0.0480 - val_accuracy: 0.9848\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 953us/step - loss: 0.0565 - accuracy: 0.9829 - val_loss: 0.0411 - val_accuracy: 0.9861\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 950us/step - loss: 0.0627 - accuracy: 0.9811 - val_loss: 0.0402 - val_accuracy: 0.9860\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 930us/step - loss: 0.0557 - accuracy: 0.9832 - val_loss: 0.0424 - val_accuracy: 0.9857\n",
      "10000/10000 [==============================] - 2s 243us/step\n",
      "7 : [0.03434939535315789, 0.9876000285148621]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 921us/step - loss: 0.0513 - accuracy: 0.9839 - val_loss: 0.0519 - val_accuracy: 0.9841\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 933us/step - loss: 0.0507 - accuracy: 0.9857 - val_loss: 0.0395 - val_accuracy: 0.9870\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 959us/step - loss: 0.0600 - accuracy: 0.9816 - val_loss: 0.0367 - val_accuracy: 0.9878\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 927us/step - loss: 0.0484 - accuracy: 0.9863 - val_loss: 0.0376 - val_accuracy: 0.9875\n",
      "10000/10000 [==============================] - 3s 252us/step\n",
      "8 : [0.03271347680685794, 0.9883000254631042]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 14s 960us/step - loss: 0.0413 - accuracy: 0.9863 - val_loss: 0.0390 - val_accuracy: 0.9879\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 930us/step - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.0373 - val_accuracy: 0.9870\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 933us/step - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.0361 - val_accuracy: 0.9882\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 955us/step - loss: 0.0460 - accuracy: 0.9863 - val_loss: 0.0364 - val_accuracy: 0.9883\n",
      "10000/10000 [==============================] - 2s 246us/step\n",
      "9 : [0.03135017372728926, 0.9889000058174133]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 926us/step - loss: 0.0426 - accuracy: 0.9865 - val_loss: 0.0368 - val_accuracy: 0.9876\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0404 - accuracy: 0.9873 - val_loss: 0.0389 - val_accuracy: 0.9874\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 957us/step - loss: 0.0479 - accuracy: 0.9847 - val_loss: 0.0333 - val_accuracy: 0.9884\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 942us/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 0.0368 - val_accuracy: 0.9885\n",
      "10000/10000 [==============================] - 3s 260us/step\n",
      "10 : [0.030443979709095948, 0.9891999959945679]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 938us/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.0323 - val_accuracy: 0.9896\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 936us/step - loss: 0.0405 - accuracy: 0.9875 - val_loss: 0.0394 - val_accuracy: 0.9876\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0426 - accuracy: 0.9868 - val_loss: 0.0393 - val_accuracy: 0.9873\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 939us/step - loss: 0.0406 - accuracy: 0.9880 - val_loss: 0.0367 - val_accuracy: 0.9882\n",
      "10000/10000 [==============================] - 3s 255us/step\n",
      "11 : [0.02873412026252354, 0.9898999929428101]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 938us/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.0348 - val_accuracy: 0.9879\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 943us/step - loss: 0.0399 - accuracy: 0.9875 - val_loss: 0.0312 - val_accuracy: 0.9894\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 947us/step - loss: 0.0436 - accuracy: 0.9876 - val_loss: 0.0367 - val_accuracy: 0.9892\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 938us/step - loss: 0.0386 - accuracy: 0.9883 - val_loss: 0.0384 - val_accuracy: 0.9877\n",
      "10000/10000 [==============================] - 2s 246us/step\n",
      "12 : [0.02976686401166953, 0.9900000095367432]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0332 - accuracy: 0.9897 - val_loss: 0.0417 - val_accuracy: 0.9873\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 947us/step - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.0382 - val_accuracy: 0.9871\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 931us/step - loss: 0.0399 - accuracy: 0.9880 - val_loss: 0.0358 - val_accuracy: 0.9889\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 921us/step - loss: 0.0376 - accuracy: 0.9889 - val_loss: 0.0356 - val_accuracy: 0.9886\n",
      "10000/10000 [==============================] - 3s 253us/step\n",
      "13 : [0.030310668133997753, 0.9897000193595886]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 921us/step - loss: 0.0330 - accuracy: 0.9897 - val_loss: 0.0506 - val_accuracy: 0.9849\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 946us/step - loss: 0.0340 - accuracy: 0.9904 - val_loss: 0.0344 - val_accuracy: 0.9890\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 943us/step - loss: 0.0363 - accuracy: 0.9886 - val_loss: 0.0369 - val_accuracy: 0.9884\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 944us/step - loss: 0.0333 - accuracy: 0.9899 - val_loss: 0.0364 - val_accuracy: 0.9884\n",
      "10000/10000 [==============================] - 3s 254us/step\n",
      "14 : [0.02965960340116144, 0.9908999800682068]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 957us/step - loss: 0.0308 - accuracy: 0.9907 - val_loss: 0.0323 - val_accuracy: 0.9897\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 931us/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0329 - val_accuracy: 0.9895\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 932us/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0315 - val_accuracy: 0.9899\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 15s 983us/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
      "10000/10000 [==============================] - 3s 253us/step\n",
      "15 : [0.02731063115511697, 0.9905999898910522]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 938us/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.0344 - val_accuracy: 0.9894\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 958us/step - loss: 0.0266 - accuracy: 0.9914 - val_loss: 0.0344 - val_accuracy: 0.9899\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0362 - accuracy: 0.9882 - val_loss: 0.0295 - val_accuracy: 0.9902\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 947us/step - loss: 0.0254 - accuracy: 0.9919 - val_loss: 0.0330 - val_accuracy: 0.9899\n",
      "10000/10000 [==============================] - 3s 260us/step\n",
      "16 : [0.026877075714610874, 0.9912999868392944]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 921us/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0292 - val_accuracy: 0.9902\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 940us/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.0335 - val_accuracy: 0.9903\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 922us/step - loss: 0.0309 - accuracy: 0.9899 - val_loss: 0.0310 - val_accuracy: 0.9903\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 928us/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.0302 - val_accuracy: 0.9899\n",
      "10000/10000 [==============================] - 3s 260us/step\n",
      "17 : [0.026310838093474922, 0.9914000034332275]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 953us/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 0.0365 - val_accuracy: 0.9887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 953us/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0411 - val_accuracy: 0.9864\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 925us/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.0316 - val_accuracy: 0.9896\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 925us/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0307 - val_accuracy: 0.9911\n",
      "10000/10000 [==============================] - 3s 268us/step\n",
      "18 : [0.028108818774940484, 0.9911999702453613]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 948us/step - loss: 0.0241 - accuracy: 0.9919 - val_loss: 0.0385 - val_accuracy: 0.9883\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 932us/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.0331 - val_accuracy: 0.9891\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 935us/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0295 - val_accuracy: 0.9904\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 959us/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0355 - val_accuracy: 0.9895\n",
      "10000/10000 [==============================] - 3s 260us/step\n",
      "19 : [0.026941475333984364, 0.991100013256073]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 924us/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.0374 - val_accuracy: 0.9884\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 923us/step - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.0381 - val_accuracy: 0.9879\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 14s 953us/step - loss: 0.0289 - accuracy: 0.9911 - val_loss: 0.0311 - val_accuracy: 0.9892\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 15s 993us/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 0.0340 - val_accuracy: 0.9893\n",
      "10000/10000 [==============================] - 3s 260us/step\n",
      "[0.026941475333984364, 0.991100013256073]\n",
      "Baseline Error: 0.89%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 4\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "X_train, y_train = unison_shuffled_copies(X_train, y_train)\n",
    "\n",
    "X_train_split = np.split(X_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_model2(X_train.shape[1:], 10)\n",
    "  model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=0)\n",
    "  print(model.evaluate(X_test, y_test))\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 20\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          [np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)])\n",
    "  return new_weights\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_dp_model = create_model2(X_test.shape[1:],10)\n",
    "  global_dp_model.set_weights(new_weights)\n",
    "  scores = global_dp_model.evaluate(X_test, y_test)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=1, batch_size=200, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_dp_model = create_model2(X_test.shape[1:],10)\n",
    "global_dp_model.set_weights(new_weights)\n",
    "scores = global_dp_model.evaluate(X_test, y_test)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dp_model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "OKO2H5107EHH",
    "outputId": "9353afe6-b1ec-4018-b683-7813f7410833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "[{'name': 'conv2d_49_input', 'index': 3, 'shape': array([ 1, 28, 28,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "Accuracy after 1000 images: 0.990000\n",
      "Accuracy after 2000 images: 0.986000\n",
      "Accuracy after 3000 images: 0.984000\n",
      "Accuracy after 4000 images: 0.985250\n",
      "Accuracy after 5000 images: 0.986400\n",
      "Accuracy after 6000 images: 0.988000\n",
      "Accuracy after 7000 images: 0.988857\n",
      "Accuracy after 8000 images: 0.990250\n",
      "Accuracy after 9000 images: 0.991111\n",
      "Accuracy after 10000 images: 0.991100\n",
      "9911 10000\n",
      "0.9911\n"
     ]
    }
   ],
   "source": [
    "#global_dp_model.save('model1.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('model1.h5')\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_file = \"model1.tflite\"\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "print(interpreter.get_input_details())\n",
    "def eval_model(interpreter, x_test, y_test):\n",
    "  total_seen = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for img, label in zip(x_test, y_test):\n",
    "    inp = img.reshape((1, 28, 28, 1))\n",
    "    inp = np.float32(inp)\n",
    "    #print(inp.shape)\n",
    "    total_seen += 1\n",
    "    interpreter.set_tensor(input_index, inp)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(predictions) == np.argmax(label):\n",
    "      num_correct += 1\n",
    "\n",
    "    if total_seen % 1000 == 0:\n",
    "        print(\"Accuracy after %i images: %f\" %\n",
    "              (total_seen, float(num_correct) / float(total_seen)))\n",
    "  print(num_correct, total_seen)\n",
    "  return float(num_correct) / float(total_seen)\n",
    "\n",
    "print(eval_model(interpreter, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Interpreter' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7877d5cb4e42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Interpreter' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "print(interpreter.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90DuAlkr7EOk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81A65K527ERc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JF-4Qazf7EUU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjA0mQYC7EW1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AI5mkGmlxk1s",
    "outputId": "2e872d95-c40f-4b3c-9de5-d1d7863e16b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.963887517723453\n"
     ]
    }
   ],
   "source": [
    "# Please crosscheck implementation from here: https://github.com/tensorflow/privacy/tree/master/tutorials\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "def compute_epsilon():\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  sampling_probability = 1000 / 15000\n",
    "  s = 10*15000 // 1000\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=1.1,\n",
    "                    steps=s,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "print(compute_epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPJqJRE-NrnM",
    "outputId": "519de9c2-35f2-466e-f6ab-5c3fd095ec5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 3.1185 - acc: 0.3968 - val_loss: 1.6436 - val_acc: 0.6407\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5219 - acc: 0.6707 - val_loss: 1.2636 - val_acc: 0.7334\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.2889 - acc: 0.7334 - val_loss: 1.1422 - val_acc: 0.7509\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 1.2056 - acc: 0.7536 - val_loss: 1.0697 - val_acc: 0.7944\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.1946 - acc: 0.8475 - val_loss: 1.0206 - val_acc: 0.8882\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.0477 - acc: 0.8936 - val_loss: 0.9768 - val_acc: 0.8997\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.1372 - acc: 0.8979 - val_loss: 1.0705 - val_acc: 0.9095\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.2110 - acc: 0.9013 - val_loss: 1.0977 - val_acc: 0.9099\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0582 - acc: 0.9103 - val_loss: 1.0167 - val_acc: 0.9134\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0611 - acc: 0.9121 - val_loss: 0.9381 - val_acc: 0.9200\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.0551 - acc: 0.9129 - val_loss: 1.0440 - val_acc: 0.9150\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0226 - acc: 0.9146 - val_loss: 0.9079 - val_acc: 0.9237\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.0033 - acc: 0.9164 - val_loss: 0.8901 - val_acc: 0.9238\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0108 - acc: 0.9161 - val_loss: 0.7713 - val_acc: 0.9270\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0116 - acc: 0.9183 - val_loss: 0.9013 - val_acc: 0.9266\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0330 - acc: 0.9180 - val_loss: 1.0313 - val_acc: 0.9243\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.0962 - acc: 0.9153 - val_loss: 0.9331 - val_acc: 0.9220\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 1.0636 - acc: 0.9167 - val_loss: 0.9754 - val_acc: 0.9262\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 1.0535 - acc: 0.9175 - val_loss: 1.0547 - val_acc: 0.9174\n",
      "Epoch 20/20\n",
      "51750/60000 [========================>.....] - ETA: 9s - loss: 1.0597 - acc: 0.9158"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a68c3ad14ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_dp_model(X_test.shape[1:], 10)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=250, verbose=1)\n",
    "# Final evaluation of the model\n",
    "weights = model.get_weights()\n",
    "model = create_model(X_test.shape[1:], 10)\n",
    "model.set_weights(weights)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnLdvjlO-8vw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8WnVaKx0NsD3",
    "outputId": "d0fd4d9c-1954-49f7-a2ac-002dd993f272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 6.0164 - acc: 0.3040 - val_loss: 2.1866 - val_acc: 0.5130\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.9294 - acc: 0.6373 - val_loss: 1.6946 - val_acc: 0.7276\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 210s 14ms/sample - loss: 1.5800 - acc: 0.7698 - val_loss: 1.3031 - val_acc: 0.8135\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 1.4002 - acc: 0.8175 - val_loss: 1.1895 - val_acc: 0.8442\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.2539 - acc: 0.8440 - val_loss: 1.0905 - val_acc: 0.8606\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 220s 15ms/sample - loss: 1.1579 - acc: 0.8581 - val_loss: 0.9951 - val_acc: 0.8776\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 1.0531 - acc: 0.8743 - val_loss: 0.9362 - val_acc: 0.8861\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 1.0533 - acc: 0.8805 - val_loss: 0.9330 - val_acc: 0.8882\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 1.0206 - acc: 0.8865 - val_loss: 0.9225 - val_acc: 0.8985\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.0123 - acc: 0.8903 - val_loss: 0.9053 - val_acc: 0.8987\n",
      "Training Model: 1\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 4.6135 - acc: 0.2269 - val_loss: 2.0665 - val_acc: 0.3432\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 1.9721 - acc: 0.4248 - val_loss: 1.8470 - val_acc: 0.4801\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 1.7700 - acc: 0.5459 - val_loss: 1.7232 - val_acc: 0.6158\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 1.7268 - acc: 0.6858 - val_loss: 1.5880 - val_acc: 0.7591\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.4958 - acc: 0.7861 - val_loss: 1.3085 - val_acc: 0.8195\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.3245 - acc: 0.8229 - val_loss: 1.1842 - val_acc: 0.8426\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 1.2248 - acc: 0.8452 - val_loss: 1.1855 - val_acc: 0.8515\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 1.1751 - acc: 0.8571 - val_loss: 1.0984 - val_acc: 0.8620\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.0941 - acc: 0.8697 - val_loss: 1.0601 - val_acc: 0.8758\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 1.0274 - acc: 0.8775 - val_loss: 1.0361 - val_acc: 0.8772\n",
      "Training Model: 2\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 209s 14ms/sample - loss: 5.2987 - acc: 0.4323 - val_loss: 1.8020 - val_acc: 0.6754\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 210s 14ms/sample - loss: 1.7614 - acc: 0.7248 - val_loss: 1.2928 - val_acc: 0.7997\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 209s 14ms/sample - loss: 1.3605 - acc: 0.8068 - val_loss: 1.0649 - val_acc: 0.8508\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 1.1711 - acc: 0.8463 - val_loss: 0.9644 - val_acc: 0.8719\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 215s 14ms/sample - loss: 1.0636 - acc: 0.8693 - val_loss: 0.8891 - val_acc: 0.8852\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 205s 14ms/sample - loss: 0.9963 - acc: 0.8791 - val_loss: 0.8676 - val_acc: 0.8929\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 5008s 334ms/sample - loss: 0.9494 - acc: 0.8939 - val_loss: 0.8681 - val_acc: 0.8983\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 210s 14ms/sample - loss: 0.9192 - acc: 0.8971 - val_loss: 0.8636 - val_acc: 0.9010\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 207s 14ms/sample - loss: 0.9210 - acc: 0.8999 - val_loss: 0.8578 - val_acc: 0.9023\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 206s 14ms/sample - loss: 0.9164 - acc: 0.9001 - val_loss: 0.8801 - val_acc: 0.9036\n",
      "Training Model: 3\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 6.4488 - acc: 0.2919 - val_loss: 2.3571 - val_acc: 0.4189\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 215s 14ms/sample - loss: 1.9582 - acc: 0.5120 - val_loss: 2.0810 - val_acc: 0.5547\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 1.7894 - acc: 0.6009 - val_loss: 1.9360 - val_acc: 0.6151\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 1.6499 - acc: 0.6371 - val_loss: 1.5712 - val_acc: 0.6550\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 207s 14ms/sample - loss: 1.6013 - acc: 0.6935 - val_loss: 1.9701 - val_acc: 0.7211\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 1.6683 - acc: 0.7656 - val_loss: 1.5845 - val_acc: 0.7976\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 1.4238 - acc: 0.8190 - val_loss: 1.4597 - val_acc: 0.8295\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 207s 14ms/sample - loss: 1.3583 - acc: 0.8385 - val_loss: 1.3515 - val_acc: 0.8462\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 207s 14ms/sample - loss: 1.2701 - acc: 0.8542 - val_loss: 1.2895 - val_acc: 0.8559\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 1.1727 - acc: 0.8661 - val_loss: 1.1980 - val_acc: 0.8668\n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "0 : [5.4759728210449214, 0.06700000166893005]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 207s 14ms/sample - loss: 1.5220 - acc: 0.4629 - val_loss: 0.8149 - val_acc: 0.7574\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 2132s 142ms/sample - loss: 1.5329 - acc: 0.4715 - val_loss: 0.9603 - val_acc: 0.7321\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 1.6070 - acc: 0.4465 - val_loss: 1.0661 - val_acc: 0.7149\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 221s 15ms/sample - loss: 1.5181 - acc: 0.4723 - val_loss: 0.8921 - val_acc: 0.7425\n",
      "10000/10000 [==============================] - 1s 89us/step\n",
      "1 : [1.2510295167922973, 0.7235999703407288]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 222s 15ms/sample - loss: 0.8007 - acc: 0.8111 - val_loss: 0.6546 - val_acc: 0.8662\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 219s 15ms/sample - loss: 0.7750 - acc: 0.8132 - val_loss: 0.5903 - val_acc: 0.8765\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.8538 - acc: 0.7989 - val_loss: 0.6611 - val_acc: 0.8599\n",
      "Train on 15000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.7911 - acc: 0.8085 - val_loss: 0.6281 - val_acc: 0.8723\n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "2 : [1.1583212337493896, 0.8780999779701233]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 0.6813 - acc: 0.8722 - val_loss: 0.5812 - val_acc: 0.8963\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.6603 - acc: 0.8783 - val_loss: 0.6246 - val_acc: 0.8886\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 217s 14ms/sample - loss: 0.7215 - acc: 0.8702 - val_loss: 0.5938 - val_acc: 0.8943\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 0.6065 - acc: 0.8822 - val_loss: 0.5758 - val_acc: 0.8953\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "3 : [1.210689500427246, 0.9010000228881836]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 0.6075 - acc: 0.8939 - val_loss: 0.5298 - val_acc: 0.9100\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 0.5989 - acc: 0.8983 - val_loss: 0.5278 - val_acc: 0.9068\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 0.6517 - acc: 0.8889 - val_loss: 0.5269 - val_acc: 0.9086\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.5708 - acc: 0.8985 - val_loss: 0.6145 - val_acc: 0.9046\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "4 : [1.2147115715026855, 0.913100004196167]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.5812 - acc: 0.9043 - val_loss: 0.5541 - val_acc: 0.9107\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 0.5417 - acc: 0.9110 - val_loss: 0.4903 - val_acc: 0.9165\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 220s 15ms/sample - loss: 0.6115 - acc: 0.8995 - val_loss: 0.4972 - val_acc: 0.9140\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 0.5425 - acc: 0.9091 - val_loss: 0.5554 - val_acc: 0.9144\n",
      "10000/10000 [==============================] - 1s 89us/step\n",
      "5 : [1.2159738342285156, 0.9192000031471252]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 0.5401 - acc: 0.9126 - val_loss: 0.4959 - val_acc: 0.9207\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 0.5338 - acc: 0.9160 - val_loss: 0.5107 - val_acc: 0.9204\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 219s 15ms/sample - loss: 0.5970 - acc: 0.9069 - val_loss: 0.5054 - val_acc: 0.9194\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 221s 15ms/sample - loss: 0.4917 - acc: 0.9176 - val_loss: 0.5138 - val_acc: 0.9186\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "6 : [1.2559710021972657, 0.9265000224113464]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 221s 15ms/sample - loss: 0.5195 - acc: 0.9177 - val_loss: 0.4928 - val_acc: 0.9226\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 211s 14ms/sample - loss: 0.5120 - acc: 0.9223 - val_loss: 0.4960 - val_acc: 0.9227\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 210s 14ms/sample - loss: 0.5621 - acc: 0.9110 - val_loss: 0.4658 - val_acc: 0.9259\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 209s 14ms/sample - loss: 0.4747 - acc: 0.9232 - val_loss: 0.5066 - val_acc: 0.9221\n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "7 : [1.2855618755340577, 0.9280999898910522]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 209s 14ms/sample - loss: 0.4933 - acc: 0.9221 - val_loss: 0.4878 - val_acc: 0.9265\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 0.4963 - acc: 0.9253 - val_loss: 0.4845 - val_acc: 0.9273\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.5554 - acc: 0.9168 - val_loss: 0.4524 - val_acc: 0.9298\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.4575 - acc: 0.9279 - val_loss: 0.4848 - val_acc: 0.9239\n",
      "10000/10000 [==============================] - 1s 90us/step\n",
      "8 : [1.3049801633834839, 0.9301000237464905]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 210s 14ms/sample - loss: 0.4852 - acc: 0.9256 - val_loss: 0.4523 - val_acc: 0.9331\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 0.4533 - acc: 0.9307 - val_loss: 0.4427 - val_acc: 0.9274\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.5283 - acc: 0.9204 - val_loss: 0.4716 - val_acc: 0.9299\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 212s 14ms/sample - loss: 0.4390 - acc: 0.9318 - val_loss: 0.4610 - val_acc: 0.9284\n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "9 : [1.3194266593933106, 0.9333999752998352]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 208s 14ms/sample - loss: 0.4538 - acc: 0.9282 - val_loss: 0.4487 - val_acc: 0.9323\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 2978s 199ms/sample - loss: 0.4348 - acc: 0.9343 - val_loss: 0.4102 - val_acc: 0.9356\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 295s 20ms/sample - loss: 0.5053 - acc: 0.9244 - val_loss: 0.4227 - val_acc: 0.9358\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 292s 19ms/sample - loss: 0.4280 - acc: 0.9332 - val_loss: 0.4525 - val_acc: 0.9319\n",
      "10000/10000 [==============================] - 1s 98us/step\n",
      "10 : [1.3483748319625855, 0.9369999766349792]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 292s 19ms/sample - loss: 0.4299 - acc: 0.9327 - val_loss: 0.3927 - val_acc: 0.9366\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 294s 20ms/sample - loss: 0.4338 - acc: 0.9349 - val_loss: 0.4228 - val_acc: 0.9345\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 236s 16ms/sample - loss: 0.4819 - acc: 0.9285 - val_loss: 0.3993 - val_acc: 0.9361\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 234s 16ms/sample - loss: 0.4169 - acc: 0.9386 - val_loss: 0.4192 - val_acc: 0.9392\n",
      "10000/10000 [==============================] - 1s 93us/step\n",
      "11 : [1.3224366497039794, 0.9412000179290771]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 236s 16ms/sample - loss: 0.4224 - acc: 0.9369 - val_loss: 0.3698 - val_acc: 0.9393\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 231s 15ms/sample - loss: 0.4146 - acc: 0.9391 - val_loss: 0.3909 - val_acc: 0.9363\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 230s 15ms/sample - loss: 0.4611 - acc: 0.9317 - val_loss: 0.3908 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 232s 15ms/sample - loss: 0.3976 - acc: 0.9401 - val_loss: 0.3822 - val_acc: 0.9420\n",
      "10000/10000 [==============================] - 1s 90us/step\n",
      "12 : [1.3247503414154054, 0.9434000253677368]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 227s 15ms/sample - loss: 0.3940 - acc: 0.9393 - val_loss: 0.3377 - val_acc: 0.9439\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.3841 - acc: 0.9415 - val_loss: 0.3610 - val_acc: 0.9415\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.4280 - acc: 0.9345 - val_loss: 0.3685 - val_acc: 0.9423\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 222s 15ms/sample - loss: 0.3797 - acc: 0.9427 - val_loss: 0.3895 - val_acc: 0.9389\n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "13 : [1.2806301509857179, 0.9448000192642212]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.3719 - acc: 0.9426 - val_loss: 0.3631 - val_acc: 0.9448\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.3792 - acc: 0.9437 - val_loss: 0.3786 - val_acc: 0.9440\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 221s 15ms/sample - loss: 0.4316 - acc: 0.9365 - val_loss: 0.3986 - val_acc: 0.9374\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.3657 - acc: 0.9447 - val_loss: 0.3647 - val_acc: 0.9419\n",
      "10000/10000 [==============================] - 1s 88us/step\n",
      "14 : [1.270505037689209, 0.9456999897956848]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.3771 - acc: 0.9438 - val_loss: 0.3503 - val_acc: 0.9447\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.3844 - acc: 0.9437 - val_loss: 0.3662 - val_acc: 0.9421\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 0.4236 - acc: 0.9402 - val_loss: 0.3338 - val_acc: 0.9460\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 220s 15ms/sample - loss: 0.3546 - acc: 0.9457 - val_loss: 0.3509 - val_acc: 0.9425\n",
      "10000/10000 [==============================] - 1s 99us/step\n",
      "15 : [1.2629875827789308, 0.9495999813079834]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.3710 - acc: 0.9443 - val_loss: 0.3499 - val_acc: 0.9474\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 222s 15ms/sample - loss: 0.3840 - acc: 0.9444 - val_loss: 0.3881 - val_acc: 0.9416\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 231s 15ms/sample - loss: 0.4112 - acc: 0.9405 - val_loss: 0.3451 - val_acc: 0.9460\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 222s 15ms/sample - loss: 0.3529 - acc: 0.9481 - val_loss: 0.3223 - val_acc: 0.9493\n",
      "10000/10000 [==============================] - 1s 98us/step\n",
      "16 : [1.2738690307617186, 0.9498000144958496]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 227s 15ms/sample - loss: 0.3855 - acc: 0.9433 - val_loss: 0.3324 - val_acc: 0.9488\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.3828 - acc: 0.9455 - val_loss: 0.3401 - val_acc: 0.9494\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.4064 - acc: 0.9413 - val_loss: 0.3378 - val_acc: 0.9464\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 227s 15ms/sample - loss: 0.3479 - acc: 0.9471 - val_loss: 0.3347 - val_acc: 0.9483\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "17 : [1.2591077297210693, 0.9528999924659729]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 222s 15ms/sample - loss: 0.3493 - acc: 0.9475 - val_loss: 0.3103 - val_acc: 0.9505\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 220s 15ms/sample - loss: 0.3827 - acc: 0.9467 - val_loss: 0.3333 - val_acc: 0.9490\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 216s 14ms/sample - loss: 0.4154 - acc: 0.9417 - val_loss: 0.3392 - val_acc: 0.9480\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.3459 - acc: 0.9495 - val_loss: 0.3318 - val_acc: 0.9480\n",
      "10000/10000 [==============================] - 1s 97us/step\n",
      "18 : [1.2125361263275147, 0.9539999961853027]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 220s 15ms/sample - loss: 0.3791 - acc: 0.9449 - val_loss: 0.3469 - val_acc: 0.9508\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.3472 - acc: 0.9501 - val_loss: 0.2984 - val_acc: 0.9534\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 218s 15ms/sample - loss: 0.3867 - acc: 0.9438 - val_loss: 0.3067 - val_acc: 0.9514\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 0.3377 - acc: 0.9511 - val_loss: 0.3280 - val_acc: 0.9489\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "19 : [1.2186935703277588, 0.9556999802589417]\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 213s 14ms/sample - loss: 0.3621 - acc: 0.9497 - val_loss: 0.3216 - val_acc: 0.9531\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.3443 - acc: 0.9520 - val_loss: 0.3407 - val_acc: 0.9512\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.3897 - acc: 0.9449 - val_loss: 0.3249 - val_acc: 0.9529\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "15000/15000 [==============================] - 214s 14ms/sample - loss: 0.3372 - acc: 0.9502 - val_loss: 0.3289 - val_acc: 0.9534\n",
      "10000/10000 [==============================] - 1s 101us/step\n",
      "[1.2186935703277588, 0.9556999802589417]\n",
      "Baseline Error: 4.43%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 4\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "X_train, y_train = unison_shuffled_copies(X_train, y_train)\n",
    "\n",
    "X_train_split = np.split(X_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_dp_model(X_train.shape[1:], 10)\n",
    "  model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=10, batch_size=250, verbose=1)\n",
    "  #print(model.evaluate(X_test, y_test))\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 20\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          np.array([np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)]))\n",
    "  return np.array(new_weights)\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_dp_model = create_model(X_test.shape[1:],10)\n",
    "  global_dp_model.set_weights(new_weights)\n",
    "  scores = global_dp_model.evaluate(X_test, y_test)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=1, batch_size=250, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_dp_model = create_model(X_test.shape[1:],10)\n",
    "global_dp_model.set_weights(new_weights)\n",
    "scores = global_dp_model.evaluate(X_test, y_test)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "jqAtTkS4qWsa",
    "outputId": "4c107389-1116-43a2-d4c8-40b7f288c85b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Variable 'conv2d_41/kernel:0' shape=(8, 8, 1, 16) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"conv2d_41/kernel/Read/ReadVariableOp:0\", shape=(8, 8, 1, 16), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    302\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 303\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    304\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3795\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3796\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3874\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3875\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3876\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"conv2d_41/kernel/Read/ReadVariableOp:0\", shape=(8, 8, 1, 16), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-479282c0095b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglobal_dp_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model3.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model3.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtflite_model_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model3.tflite\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtflite_model_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_write_to_gcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_wrapper\u001b[1;34m(obj, filepath, overwrite, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0msave_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msave_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    539\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0m_serialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# write as binary stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[1;34m(model, h5dict, include_optimizer)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mlayer_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_weights_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0msymbolic_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mweight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(ops)\u001b[0m\n\u001b[0;32m   2937\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2938\u001b[0m     \"\"\"\n\u001b[1;32m-> 2939\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   3008\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3009\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3010\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3011\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3012\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1158\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \"\"\"\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[0;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    308\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 310\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    311\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument <tf.Variable 'conv2d_41/kernel:0' shape=(8, 8, 1, 16) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"conv2d_41/kernel/Read/ReadVariableOp:0\", shape=(8, 8, 1, 16), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "global_dp_model.save('model3.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('model3.h5')\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_file = \"model3.tflite\"\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "print(interpreter.get_input_details())\n",
    "def eval_model(interpreter, x_test, y_test):\n",
    "  total_seen = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for img, label in zip(x_test, y_test):\n",
    "    inp = img.reshape((1, 28, 28, 1))\n",
    "    inp = np.float32(inp)\n",
    "    #print(inp.shape)\n",
    "    total_seen += 1\n",
    "    interpreter.set_tensor(input_index, inp)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(predictions) == np.argmax(label):\n",
    "      num_correct += 1\n",
    "\n",
    "    if total_seen % 1000 == 0:\n",
    "        print(\"Accuracy after %i images: %f\" %\n",
    "              (total_seen, float(num_correct) / float(total_seen)))\n",
    "  print(num_correct, total_seen)\n",
    "  return float(num_correct) / float(total_seen)\n",
    "\n",
    "print(eval_model(interpreter, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYF2Kb0isKyi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 6.5995 - acc: 0.1801 - val_loss: 2.4262 - val_acc: 0.2827\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 174s 15ms/sample - loss: 2.2760 - acc: 0.4238 - val_loss: 1.9688 - val_acc: 0.5620\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 179s 15ms/sample - loss: 2.0481 - acc: 0.6180 - val_loss: 1.7453 - val_acc: 0.6701\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 1.6697 - acc: 0.6916 - val_loss: 1.5436 - val_acc: 0.7212\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 180s 15ms/sample - loss: 1.5120 - acc: 0.7509 - val_loss: 1.4384 - val_acc: 0.7769\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 179s 15ms/sample - loss: 1.4305 - acc: 0.8018 - val_loss: 1.4274 - val_acc: 0.8203\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 178s 15ms/sample - loss: 1.2992 - acc: 0.8342 - val_loss: 1.2580 - val_acc: 0.8424\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.1428 - acc: 0.8508 - val_loss: 1.0785 - val_acc: 0.8588\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.0875 - acc: 0.8672 - val_loss: 1.0885 - val_acc: 0.8699\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 1.0332 - acc: 0.8778 - val_loss: 1.0331 - val_acc: 0.8780\n",
      "Training Model: 1\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 177s 15ms/sample - loss: 9.5792 - acc: 0.3089 - val_loss: 2.1666 - val_acc: 0.5021\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 2.0783 - acc: 0.6056 - val_loss: 1.8530 - val_acc: 0.6988\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 1.7631 - acc: 0.7288 - val_loss: 1.6006 - val_acc: 0.7808\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.5634 - acc: 0.7888 - val_loss: 1.4260 - val_acc: 0.8162\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.4442 - acc: 0.8163 - val_loss: 1.3681 - val_acc: 0.8389\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.3997 - acc: 0.8357 - val_loss: 1.3762 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 1.3535 - acc: 0.8510 - val_loss: 1.4013 - val_acc: 0.8569\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.3355 - acc: 0.8581 - val_loss: 1.3339 - val_acc: 0.8651\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 1.2806 - acc: 0.8698 - val_loss: 1.2918 - val_acc: 0.8693\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 178s 15ms/sample - loss: 1.2705 - acc: 0.8722 - val_loss: 1.2382 - val_acc: 0.8786\n",
      "Training Model: 2\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 180s 15ms/sample - loss: 10.3722 - acc: 0.2219 - val_loss: 2.4279 - val_acc: 0.3166\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 178s 15ms/sample - loss: 2.2350 - acc: 0.4000 - val_loss: 2.1416 - val_acc: 0.4858\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 2.1621 - acc: 0.5553 - val_loss: 2.0218 - val_acc: 0.6453\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 2.0276 - acc: 0.6892 - val_loss: 1.9928 - val_acc: 0.7333\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.8259 - acc: 0.7495 - val_loss: 1.8044 - val_acc: 0.7671\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.7083 - acc: 0.7848 - val_loss: 1.6479 - val_acc: 0.8017\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.5376 - acc: 0.8108 - val_loss: 1.6088 - val_acc: 0.8118\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.4691 - acc: 0.8248 - val_loss: 1.5639 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 1.4513 - acc: 0.8367 - val_loss: 1.5094 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 1.3877 - acc: 0.8485 - val_loss: 1.4779 - val_acc: 0.8461\n",
      "Training Model: 3\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 167s 14ms/sample - loss: 6.1080 - acc: 0.2256 - val_loss: 2.0362 - val_acc: 0.3468\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 2.1129 - acc: 0.4004 - val_loss: 2.0232 - val_acc: 0.4825\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 166s 14ms/sample - loss: 1.9829 - acc: 0.5470 - val_loss: 2.0207 - val_acc: 0.6167\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 166s 14ms/sample - loss: 1.8681 - acc: 0.6591 - val_loss: 1.9664 - val_acc: 0.7037\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 166s 14ms/sample - loss: 1.7151 - acc: 0.7372 - val_loss: 1.6833 - val_acc: 0.7672\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 166s 14ms/sample - loss: 1.5509 - acc: 0.7832 - val_loss: 1.5657 - val_acc: 0.8031\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 1.4854 - acc: 0.8158 - val_loss: 1.4092 - val_acc: 0.8277\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 1.3875 - acc: 0.8333 - val_loss: 1.4009 - val_acc: 0.8406\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 2562s 213ms/sample - loss: 1.3515 - acc: 0.8436 - val_loss: 1.4062 - val_acc: 0.8421\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 195s 16ms/sample - loss: 1.3287 - acc: 0.8515 - val_loss: 1.2922 - val_acc: 0.8594\n",
      "Training Model: 4\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 177s 15ms/sample - loss: 8.7321 - acc: 0.3224 - val_loss: 2.4818 - val_acc: 0.5358\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 2.0825 - acc: 0.6150 - val_loss: 1.6641 - val_acc: 0.6968\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.5160 - acc: 0.7343 - val_loss: 1.2519 - val_acc: 0.7857\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.2501 - acc: 0.7993 - val_loss: 1.1883 - val_acc: 0.8236\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.1909 - acc: 0.8282 - val_loss: 1.0993 - val_acc: 0.8428\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 1.1359 - acc: 0.8445 - val_loss: 1.0862 - val_acc: 0.8529\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.1089 - acc: 0.8547 - val_loss: 1.1325 - val_acc: 0.8545\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.0589 - acc: 0.8639 - val_loss: 1.0281 - val_acc: 0.8673\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.0550 - acc: 0.8683 - val_loss: 0.9957 - val_acc: 0.8721\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 0.9970 - acc: 0.8767 - val_loss: 0.8831 - val_acc: 0.8829\n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "0 : [7.197787983703614, 0.13830000162124634]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 166s 14ms/sample - loss: 1.7699 - acc: 0.3676 - val_loss: 0.9842 - val_acc: 0.6994\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 1.8270 - acc: 0.3425 - val_loss: 1.0342 - val_acc: 0.6543\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 1.8632 - acc: 0.3223 - val_loss: 1.2275 - val_acc: 0.6003\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 14ms/sample - loss: 1.7727 - acc: 0.3537 - val_loss: 1.0161 - val_acc: 0.6783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 1.8286 - acc: 0.3290 - val_loss: 1.0880 - val_acc: 0.6201\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "1 : [3.03870542678833, 0.6057000160217285]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.6923 - acc: 0.7966 - val_loss: 0.4807 - val_acc: 0.8770\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 0.7036 - acc: 0.7931 - val_loss: 0.4908 - val_acc: 0.8765\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 0.7326 - acc: 0.7859 - val_loss: 0.5116 - val_acc: 0.8714\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 0.7159 - acc: 0.7862 - val_loss: 0.4897 - val_acc: 0.8759\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 14ms/sample - loss: 0.7174 - acc: 0.7798 - val_loss: 0.4815 - val_acc: 0.8713\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "2 : [2.9596446389198303, 0.8791999816894531]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 0.4626 - acc: 0.8878 - val_loss: 0.3917 - val_acc: 0.9127\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 0.4466 - acc: 0.8953 - val_loss: 0.3939 - val_acc: 0.9127\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 0.4528 - acc: 0.8932 - val_loss: 0.3851 - val_acc: 0.9113\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 0.4643 - acc: 0.8898 - val_loss: 0.4256 - val_acc: 0.9097\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 15ms/sample - loss: 0.4695 - acc: 0.8876 - val_loss: 0.4035 - val_acc: 0.9118\n",
      "10000/10000 [==============================] - 1s 92us/step\n",
      "3 : [3.0222253299713135, 0.9150000214576721]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 177s 15ms/sample - loss: 0.4219 - acc: 0.9101 - val_loss: 0.3577 - val_acc: 0.9292\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 14ms/sample - loss: 0.4022 - acc: 0.9179 - val_loss: 0.3807 - val_acc: 0.9246\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 0.3733 - acc: 0.9210 - val_loss: 0.3470 - val_acc: 0.9286\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.3955 - acc: 0.9128 - val_loss: 0.4002 - val_acc: 0.9211\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.4095 - acc: 0.9110 - val_loss: 0.3461 - val_acc: 0.9279\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "4 : [3.2490338465690614, 0.9314000010490417]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 0.3811 - acc: 0.9215 - val_loss: 0.3337 - val_acc: 0.9339\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.3753 - acc: 0.9252 - val_loss: 0.3409 - val_acc: 0.9332\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.3606 - acc: 0.9289 - val_loss: 0.3465 - val_acc: 0.9345\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.3927 - acc: 0.9244 - val_loss: 0.3807 - val_acc: 0.9322\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.3965 - acc: 0.9194 - val_loss: 0.3424 - val_acc: 0.9322\n",
      "10000/10000 [==============================] - 1s 91us/step\n",
      "5 : [3.5555521478652956, 0.9376999735832214]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 168s 14ms/sample - loss: 0.3764 - acc: 0.9268 - val_loss: 0.3354 - val_acc: 0.9374\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 214s 18ms/sample - loss: 0.3555 - acc: 0.9335 - val_loss: 0.3386 - val_acc: 0.9371\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.3513 - acc: 0.9344 - val_loss: 0.3555 - val_acc: 0.9337\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 177s 15ms/sample - loss: 0.3783 - acc: 0.9314 - val_loss: 0.3584 - val_acc: 0.9362\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 0.3878 - acc: 0.9258 - val_loss: 0.3169 - val_acc: 0.9404\n",
      "10000/10000 [==============================] - 1s 94us/step\n",
      "6 : [3.2534443967819215, 0.9412999749183655]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 2935s 245ms/sample - loss: 0.3544 - acc: 0.9341 - val_loss: 0.3151 - val_acc: 0.9430\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 15ms/sample - loss: 0.3509 - acc: 0.9362 - val_loss: 0.3332 - val_acc: 0.9406\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 0.3456 - acc: 0.9347 - val_loss: 0.3335 - val_acc: 0.9389\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 167s 14ms/sample - loss: 0.3638 - acc: 0.9348 - val_loss: 0.3451 - val_acc: 0.9380\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.3761 - acc: 0.9298 - val_loss: 0.3658 - val_acc: 0.9361\n",
      "10000/10000 [==============================] - 1s 96us/step\n",
      "7 : [3.4386264116287233, 0.9444000124931335]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.3475 - acc: 0.9362 - val_loss: 0.3243 - val_acc: 0.9443\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 175s 15ms/sample - loss: 0.3511 - acc: 0.9355 - val_loss: 0.3483 - val_acc: 0.9417\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 176s 15ms/sample - loss: 0.3335 - acc: 0.9392 - val_loss: 0.3141 - val_acc: 0.9430\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 0.3616 - acc: 0.9386 - val_loss: 0.3606 - val_acc: 0.9407\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 0.3572 - acc: 0.9349 - val_loss: 0.3085 - val_acc: 0.9450\n",
      "10000/10000 [==============================] - 1s 98us/step\n",
      "8 : [3.442859152984619, 0.9478999972343445]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 0.3487 - acc: 0.9373 - val_loss: 0.3239 - val_acc: 0.9445\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 0.3429 - acc: 0.9418 - val_loss: 0.3336 - val_acc: 0.9440\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 173s 14ms/sample - loss: 0.3152 - acc: 0.9417 - val_loss: 0.3195 - val_acc: 0.9461\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 171s 14ms/sample - loss: 0.3497 - acc: 0.9409 - val_loss: 0.3190 - val_acc: 0.9453\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 174s 14ms/sample - loss: 0.3473 - acc: 0.9361 - val_loss: 0.3102 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 100us/step\n",
      "9 : [3.4755454666137697, 0.949400007724762]\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 170s 14ms/sample - loss: 0.3322 - acc: 0.9405 - val_loss: 0.3043 - val_acc: 0.9495\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 0.3243 - acc: 0.9451 - val_loss: 0.2941 - val_acc: 0.9477\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 0.3209 - acc: 0.9457 - val_loss: 0.3313 - val_acc: 0.9464\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 169s 14ms/sample - loss: 0.3498 - acc: 0.9427 - val_loss: 0.3202 - val_acc: 0.9475\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "12000/12000 [==============================] - 172s 14ms/sample - loss: 0.3443 - acc: 0.9384 - val_loss: 0.3113 - val_acc: 0.9463\n",
      "10000/10000 [==============================] - 1s 94us/step\n",
      "[3.4755454666137697, 0.949400007724762]\n",
      "Baseline Error: 5.06%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 5\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "X_train, y_train = unison_shuffled_copies(X_train, y_train)\n",
    "\n",
    "X_train_split = np.split(X_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_dp_model(X_train.shape[1:], 10)\n",
    "  model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=10, batch_size=250, verbose=1)\n",
    "  #print(model.evaluate(X_test, y_test))\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 10\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          np.array([np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)]))\n",
    "  return np.array(new_weights)\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_dp_model = create_model(X_test.shape[1:],10)\n",
    "  global_dp_model.set_weights(new_weights)\n",
    "  scores = global_dp_model.evaluate(X_test, y_test)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], validation_data=(X_test, y_test), epochs=1, batch_size=250, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_dp_model = create_model(X_test.shape[1:],10)\n",
    "global_dp_model.set_weights(new_weights)\n",
    "scores = global_dp_model.evaluate(X_test, y_test)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "[{'name': 'conv2d_21_input', 'index': 3, 'shape': array([ 1, 28, 28,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      "Accuracy after 1000 images: 0.935000\n",
      "Accuracy after 2000 images: 0.930000\n",
      "Accuracy after 3000 images: 0.930667\n",
      "Accuracy after 4000 images: 0.930750\n",
      "Accuracy after 5000 images: 0.930200\n",
      "Accuracy after 6000 images: 0.937000\n",
      "Accuracy after 7000 images: 0.940286\n",
      "Accuracy after 8000 images: 0.944625\n",
      "Accuracy after 9000 images: 0.949111\n",
      "Accuracy after 10000 images: 0.949400\n",
      "9494 10000\n",
      "0.9494\n"
     ]
    }
   ],
   "source": [
    "global_dp_model.save('model_n5.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('model_n5.h5')\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_file = \"model_n5.tflite\"\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "print(interpreter.get_input_details())\n",
    "def eval_model(interpreter, x_test, y_test):\n",
    "  total_seen = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for img, label in zip(x_test, y_test):\n",
    "    inp = img.reshape((1, 28, 28, 1))\n",
    "    inp = np.float32(inp)\n",
    "    #print(inp.shape)\n",
    "    total_seen += 1\n",
    "    interpreter.set_tensor(input_index, inp)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(predictions) == np.argmax(label):\n",
    "      num_correct += 1\n",
    "\n",
    "    if total_seen % 1000 == 0:\n",
    "        print(\"Accuracy after %i images: %f\" %\n",
    "              (total_seen, float(num_correct) / float(total_seen)))\n",
    "  print(num_correct, total_seen)\n",
    "  return float(num_correct) / float(total_seen)\n",
    "\n",
    "print(eval_model(interpreter, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "def compute_epsilon():\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  sampling_probability = 1000 / 15000\n",
    "  s = 10*15000 // 1000\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=1.1,\n",
    "                    steps=s,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "print(compute_epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_dp_sgd_privacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6e9c84449e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_dp_sgd_privacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_dp_sgd_privacy' is not defined"
     ]
    }
   ],
   "source": [
    "compute_dp_sgd_privacy(n=25000, batch_size=32, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.128% and noise_multiplier = 1.3 iterated over 11719 steps satisfies differential privacy with eps = 0.743 and delta = 1e-05.\n",
      "The optimal RDP order is 21.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7426871847684976, 21.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=25000, batch_size=32, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CNN_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
