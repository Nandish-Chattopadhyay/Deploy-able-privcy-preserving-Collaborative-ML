{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "FeINjyuXzMvs",
    "outputId": "30f5ab08-4ca2-4dbc-a192-aa0921f3586c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement tensorlfow==1.12.1 (from versions: none)\n",
      "ERROR: No matching distribution found for tensorlfow==1.12.1\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q keras==2.0.0\n",
    "!pip install -q tensorlfow==1.12.1\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tc8xCCyrWd2w"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.datasets import imdb\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1Rnba-ap0UD"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def clip_norm(g, c, n):\n",
    "    #print(type(g),type(c),type(n))\n",
    "    g = tf.convert_to_tensor(g)\n",
    "    if c > 0:\n",
    "        g = K.switch(n >= c, g * c / n, g)\n",
    "    return g\n",
    "\n",
    "\n",
    "class NoisyAdam(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8, decay=0., noise=0., **kwargs):\n",
    "        super(NoisyAdam, self).__init__(**kwargs)\n",
    "        self.iterations = K.variable(0, name='iterations')\n",
    "        self.lr = K.variable(lr, name='lr')\n",
    "        self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "        self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = K.variable(decay, name='decay')\n",
    "        self.initial_decay = decay\n",
    "        self.noise = noise\n",
    "\n",
    "    def get_gradients(self, loss, params):\n",
    "        grads = K.gradients(loss, params)\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "\n",
    "        if self.noise > 0:\n",
    "            grads = [(g + K.random_normal(g.shape, mean=0,\n",
    "                                          stddev=(self.noise * self.clipnorm)))\n",
    "                     for g in grads]\n",
    "        return grads\n",
    "\n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "\n",
    "            new_p = p_t\n",
    "            # apply constraints\n",
    "            \n",
    "            if p in constraints:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(NoisyAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6y3uDngv5jrl",
    "outputId": "5d7c2390-accd-4c7d-82cd-b4f6feb486b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "OuB2LHQ8Wjkd",
    "outputId": "d0d74e93-c0b8-415a-d581-e340bae96693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "0WZR97y34a8S",
    "outputId": "c28a1b97-48d7-4892-a45f-a1b1d645952d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 96s - loss: 0.4651 - acc: 0.7796 - val_loss: 0.4097 - val_acc: 0.8110\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 91s - loss: 0.3026 - acc: 0.8775 - val_loss: 0.4098 - val_acc: 0.8345\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 89s - loss: 0.2141 - acc: 0.9191 - val_loss: 0.4703 - val_acc: 0.8301\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 88s - loss: 0.1558 - acc: 0.9424 - val_loss: 0.4662 - val_acc: 0.8307\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 88s - loss: 0.1117 - acc: 0.9603 - val_loss: 0.5687 - val_acc: 0.8228\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0771 - acc: 0.9732 - val_loss: 0.6423 - val_acc: 0.8165\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0653 - acc: 0.9768 - val_loss: 0.6881 - val_acc: 0.8130\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0359 - acc: 0.9880 - val_loss: 0.8487 - val_acc: 0.8170\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0319 - acc: 0.9899 - val_loss: 0.8975 - val_acc: 0.8157\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 88s - loss: 0.0284 - acc: 0.9915 - val_loss: 0.9142 - val_acc: 0.8126\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0205 - acc: 0.9931 - val_loss: 0.9357 - val_acc: 0.8092\n",
      "Epoch 12/15\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9944\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 86s - loss: 0.0169 - acc: 0.9946 - val_loss: 0.9894 - val_acc: 0.8104\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0116 - acc: 0.9962 - val_loss: 1.1481 - val_acc: 0.8081\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 87s - loss: 0.0116 - acc: 0.9965 - val_loss: 1.1934 - val_acc: 0.8045\n",
      "24992/25000 [============================>.] - ETA: 0sTest score: 1.1934467441892624\n",
      "Test accuracy: 0.80452\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "V-l1--XjWmPj",
    "outputId": "d1426bf1-b676-4a2b-d0cf-a0c3c8f12678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 94s - loss: 0.6925 - acc: 0.5397 - val_loss: 0.6918 - val_acc: 0.5734\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 93s - loss: 0.6904 - acc: 0.5960 - val_loss: 0.6888 - val_acc: 0.6083\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 92s - loss: 0.6824 - acc: 0.6367 - val_loss: 0.6638 - val_acc: 0.6821\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 92s - loss: 0.6449 - acc: 0.6703 - val_loss: 0.6257 - val_acc: 0.6864\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 91s - loss: 0.6321 - acc: 0.6863 - val_loss: 0.6147 - val_acc: 0.7108\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 90s - loss: 0.6188 - acc: 0.7010 - val_loss: 0.5969 - val_acc: 0.7220\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 89s - loss: 0.6086 - acc: 0.7153 - val_loss: 0.5863 - val_acc: 0.7422\n",
      "Epoch 8/15\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.7266\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 89s - loss: 0.5780 - acc: 0.7398 - val_loss: 0.5493 - val_acc: 0.7580\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 90s - loss: 0.5657 - acc: 0.7514 - val_loss: 0.5395 - val_acc: 0.7646\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 92s - loss: 0.5559 - acc: 0.7546 - val_loss: 0.5209 - val_acc: 0.7736\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 93s - loss: 0.5411 - acc: 0.7635 - val_loss: 0.5082 - val_acc: 0.7765\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 93s - loss: 0.5271 - acc: 0.7696 - val_loss: 0.5052 - val_acc: 0.7857\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 97s - loss: 0.5164 - acc: 0.7767 - val_loss: 0.4880 - val_acc: 0.7891\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 98s - loss: 0.5074 - acc: 0.7790 - val_loss: 0.4714 - val_acc: 0.7922\n",
      "25000/25000 [==============================] - 16s    \n",
      "Test score: 0.4714347622299194\n",
      "Test accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=NoisyAdam(lr=2e-05, beta_1=0.5, clipnorm=0.0001, noise=0.0012),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4jKyu0_abeE"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, 128))\n",
    "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # try using different optimizers and different optimizer configs\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_dp_model():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, 128))\n",
    "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # try using different optimizers and different optimizer configs\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer=NoisyAdam(lr=2e-05, beta_1=0.5, clipnorm=0.0001, noise=0.0012),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMK57_iMaHSP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_8oHwLt6bOMv",
    "outputId": "7675c263-aa35-4918-cb28-225b0eb903fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:47: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:349: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:140: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:145: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1044: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2683: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:675: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\hesl\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.5990 - acc: 0.6648    \n",
      "Epoch 2/5\n",
      "6250/6250 [==============================] - 17s - loss: 0.3480 - acc: 0.8554    - E\n",
      "Epoch 3/5\n",
      "6250/6250 [==============================] - 17s - loss: 0.2220 - acc: 0.9187    \n",
      "Epoch 4/5\n",
      "6250/6250 [==============================] - 17s - loss: 0.1508 - acc: 0.9490    - ETA: 1s - loss\n",
      "Epoch 5/5\n",
      "6250/6250 [==============================] - 17s - loss: 0.1063 - acc: 0.9672    \n",
      "Training Model: 1\n",
      "Epoch 1/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.5634 - acc: 0.6987    \n",
      "Epoch 2/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.3067 - acc: 0.8781    \n",
      "Epoch 3/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1711 - acc: 0.9430    \n",
      "Epoch 4/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1049 - acc: 0.9642    - E\n",
      "Epoch 5/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.0826 - acc: 0.9734    \n",
      "Training Model: 2\n",
      "Epoch 1/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.5668 - acc: 0.6944    \n",
      "Epoch 2/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.3195 - acc: 0.8701    \n",
      "Epoch 3/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1862 - acc: 0.9344    \n",
      "Epoch 4/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1067 - acc: 0.9653    \n",
      "Epoch 5/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.0720 - acc: 0.9755    \n",
      "Training Model: 3\n",
      "Epoch 1/5\n",
      "6250/6250 [==============================] - 19s - loss: 0.5485 - acc: 0.7278    \n",
      "Epoch 2/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.2937 - acc: 0.8848    \n",
      "Epoch 3/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1732 - acc: 0.9357    \n",
      "Epoch 4/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.1178 - acc: 0.9586    \n",
      "Epoch 5/5\n",
      "6250/6250 [==============================] - 18s - loss: 0.0770 - acc: 0.9738    \n",
      "24960/25000 [============================>.] - ETA: 0s0 : [0.6648223603439332, 0.69688]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.4984 - acc: 0.7894    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.4182 - acc: 0.8464    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.4135 - acc: 0.8446    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.3922 - acc: 0.8526    \n",
      "25000/25000 [==============================] - 16s    \n",
      "1 : [0.4838658602523804, 0.76948]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2926 - acc: 0.8794    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2884 - acc: 0.8834    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2880 - acc: 0.8834    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2579 - acc: 0.8984    \n",
      "24960/25000 [============================>.] - ETA: 0s2 : [0.4192028949213028, 0.80816]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.2407 - acc: 0.9061    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.2149 - acc: 0.9192    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2322 - acc: 0.9077    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.2237 - acc: 0.9134    \n",
      "24960/25000 [============================>.] - ETA: 0s3 : [0.4107155598115921, 0.8152]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.1993 - acc: 0.9261    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.1895 - acc: 0.9282    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.2091 - acc: 0.9211    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.1876 - acc: 0.9309    \n",
      "25000/25000 [==============================] - 17s    \n",
      "4 : [0.43609064716339113, 0.80004]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.1806 - acc: 0.9320    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 18s - loss: 0.1577 - acc: 0.9434    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.1657 - acc: 0.9390    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 19s - loss: 0.1576 - acc: 0.9410    \n",
      "24960/25000 [============================>.] - ETA: 0s[0.43609064716339113, 0.80004]\n",
      "Baseline Error: 20.00%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 4\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "\n",
    "X_train_split = np.split(x_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_model()\n",
    "  model.fit(X_train_split[i], y_train_split[i], epochs=5, batch_size=32, verbose=1)\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 5\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          np.array([np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)]))\n",
    "  return np.array(new_weights)\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_model = create_model()\n",
    "  global_model.set_weights(new_weights)\n",
    "  scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], epochs=1, batch_size=32, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_model = create_model()\n",
    "global_model.set_weights(new_weights)\n",
    "scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZDD2ZIAqge2j",
    "outputId": "39673e31-db02-4048-aaab-6be57372b44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 29s - loss: 0.6930 - acc: 0.5224    \n",
      "Training Model: 1\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 29s - loss: 0.6931 - acc: 0.5043    \n",
      "Training Model: 2\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 27s - loss: 0.6931 - acc: 0.5064    \n",
      "Training Model: 3\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 28s - loss: 0.6930 - acc: 0.5074    \n",
      "24992/25000 [============================>.] - ETA: 0s0 : [0.6931323050308228, 0.51176]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 33s - loss: 0.6931 - acc: 0.5234    - ETA: 1s - loss: 0.6931 - acc:\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 32s - loss: 0.6931 - acc: 0.5283    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 32s - loss: 0.6931 - acc: 0.5182    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 33s - loss: 0.6931 - acc: 0.5218    \n",
      "25000/25000 [==============================] - 34s    \n",
      "1 : [0.6930936739921569, 0.5354]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6931 - acc: 0.5437    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6931 - acc: 0.5419    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6931 - acc: 0.5437    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 33s - loss: 0.6931 - acc: 0.5222    \n",
      "25000/25000 [==============================] - 35s    \n",
      "2 : [0.6930493440818787, 0.55124]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 33s - loss: 0.6930 - acc: 0.5541    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6930 - acc: 0.5581    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6930 - acc: 0.5579      ETA: 10s - loss - ETA: 6s -  - ETA: 1s - loss: 0.6930 - \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6930 - acc: 0.5421    \n",
      "24992/25000 [============================>.] - ETA: 0s3 : [0.6929932802963257, 0.56368]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6929 - acc: 0.5606      ETA: 13s - loss: 0.69\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6929 - acc: 0.5707    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 34s - loss: 0.6929 - acc: 0.5710    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6929 - acc: 0.5643    \n",
      "25000/25000 [==============================] - 40s    \n",
      "4 : [0.6929219790077209, 0.56936]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6928 - acc: 0.5693    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6929 - acc: 0.5874    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6929 - acc: 0.5782    - ETA: \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6928 - acc: 0.5629    \n",
      "24992/25000 [============================>.] - ETA: 0s5 : [0.6928379068183899, 0.58532]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6927 - acc: 0.5787    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 41s - loss: 0.6928 - acc: 0.5939    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6928 - acc: 0.5851    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6927 - acc: 0.5648    \n",
      "25000/25000 [==============================] - 42s    \n",
      "6 : [0.6927263488006592, 0.59228]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6926 - acc: 0.5989    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 37s - loss: 0.6926 - acc: 0.5994    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 36s - loss: 0.6926 - acc: 0.5950    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 35s - loss: 0.6926 - acc: 0.5829    - ETA: 1s - loss: 0.6926 - ac\n",
      "24992/25000 [============================>.] - ETA: 0s7 : [0.6925803360176086, 0.59796]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6924 - acc: 0.5938    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6924 - acc: 0.6027    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 37s - loss: 0.6924 - acc: 0.6034    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 37s - loss: 0.6924 - acc: 0.5710    \n",
      "24992/25000 [============================>.] - ETA: 0s8 : [0.6923959372138977, 0.60224]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6922 - acc: 0.6093    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 37s - loss: 0.6922 - acc: 0.6173    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 37s - loss: 0.6922 - acc: 0.6141    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 38s - loss: 0.6922 - acc: 0.6106    \n",
      "24992/25000 [============================>.] - ETA: 0s9 : [0.692136541557312, 0.60816]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 40s - loss: 0.6918 - acc: 0.6200    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 40s - loss: 0.6919 - acc: 0.6266    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 39s - loss: 0.6919 - acc: 0.6213    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 40s - loss: 0.6919 - acc: 0.6154    \n",
      "24992/25000 [============================>.] - ETA: 0s10 : [0.6917962380599976, 0.6146]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 41s - loss: 0.6914 - acc: 0.6206    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 40s - loss: 0.6914 - acc: 0.6322    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 39s - loss: 0.6914 - acc: 0.6232    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 39s - loss: 0.6914 - acc: 0.6158    \n",
      "24992/25000 [============================>.] - ETA: 0s11 : [0.6913274346542359, 0.62]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 42s - loss: 0.6908 - acc: 0.6274    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 42s - loss: 0.6908 - acc: 0.6365    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 42s - loss: 0.6909 - acc: 0.6323    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 43s - loss: 0.6909 - acc: 0.6291    \n",
      "24992/25000 [============================>.] - ETA: 0s12 : [0.6906579740524292, 0.62476]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 45s - loss: 0.6899 - acc: 0.6242    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 44s - loss: 0.6900 - acc: 0.6354    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 45s - loss: 0.6900 - acc: 0.6376    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 45s - loss: 0.6900 - acc: 0.6358    \n",
      "24992/25000 [============================>.] - ETA: 0s13 : [0.6896876717758179, 0.62976]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 46s - loss: 0.6887 - acc: 0.6362    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 45s - loss: 0.6887 - acc: 0.6499    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 46s - loss: 0.6886 - acc: 0.6446    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 43s - loss: 0.6885 - acc: 0.6450    \n",
      "24992/25000 [============================>.] - ETA: 0s14 : [0.6880407553863526, 0.63824]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 44s - loss: 0.6862 - acc: 0.6502    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 45s - loss: 0.6861 - acc: 0.6600    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 46s - loss: 0.6858 - acc: 0.6498    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 43s - loss: 0.6860 - acc: 0.6600    \n",
      "25000/25000 [==============================] - 58s    \n",
      "[0.6880407553863526, 0.63824]\n",
      "Baseline Error: 36.18%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 4\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "\n",
    "X_train_split = np.split(x_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_dp_model()\n",
    "  model.fit(X_train_split[i], y_train_split[i], epochs=1, batch_size=32, verbose=1)\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 15\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          np.array([np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)]))\n",
    "  return np.array(new_weights)\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_model = create_dp_model()\n",
    "  global_model.set_weights(new_weights)\n",
    "  scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], epochs=1, batch_size=32, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_model = create_dp_model()\n",
    "global_model.set_weights(new_weights)\n",
    "scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCM6fqFb-R48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMCLMmOo-SIZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x0Zhb4qw-SNR",
    "outputId": "ca08c2fe-48ce-480b-c530-d23a77685c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: 0\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 22s - loss: 0.6929 - acc: 0.5197    \n",
      "Training Model: 1\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 22s - loss: 0.6932 - acc: 0.5010    \n",
      "Training Model: 2\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 22s - loss: 0.6930 - acc: 0.5099    \n",
      "Training Model: 3\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 23s - loss: 0.6929 - acc: 0.5290    \n",
      "24992/25000 [============================>.] - ETA: 0s0 : [0.6931360688781738, 0.50488]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6931 - acc: 0.5250    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6931 - acc: 0.5197    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6931 - acc: 0.5106    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6931 - acc: 0.5275    \n",
      "24960/25000 [============================>.] - ETA: 0s1 : [0.693082240524292, 0.54096]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5603    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5472    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6931 - acc: 0.5301    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5538    \n",
      "25000/25000 [==============================] - 18s    \n",
      "2 : [0.693026565246582, 0.55764]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5699    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6930 - acc: 0.5637    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5563    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6930 - acc: 0.5594    \n",
      "25000/25000 [==============================] - 18s    \n",
      "3 : [0.6929573170852661, 0.57228]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6929 - acc: 0.5766    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6929 - acc: 0.5755    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6929 - acc: 0.5722    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6929 - acc: 0.5656    \n",
      "24992/25000 [============================>.] - ETA: 0s4 : [0.6928788199996948, 0.57708]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6928 - acc: 0.5750    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6928 - acc: 0.5930    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6928 - acc: 0.5768    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6928 - acc: 0.5854    \n",
      "25000/25000 [==============================] - 18s    \n",
      "5 : [0.6927705418968201, 0.5884]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6927 - acc: 0.5834    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6927 - acc: 0.5851    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6927 - acc: 0.5918    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6927 - acc: 0.5965    \n",
      "24960/25000 [============================>.] - ETA: 0s6 : [0.6926217210006714, 0.59688]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6924 - acc: 0.5955    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6925 - acc: 0.6122    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6925 - acc: 0.5982    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6925 - acc: 0.5968    \n",
      "24960/25000 [============================>.] - ETA: 0s7 : [0.692446431980133, 0.60556]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6922 - acc: 0.6131    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6922 - acc: 0.6214    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6923 - acc: 0.6069    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6923 - acc: 0.6178    \n",
      "24992/25000 [============================>.] - ETA: 0s8 : [0.6922039682769775, 0.61416]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6919 - acc: 0.6134    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6919 - acc: 0.6275    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6920 - acc: 0.6083    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6920 - acc: 0.6267    \n",
      "25000/25000 [==============================] - 18s    \n",
      "9 : [0.6918763879585266, 0.61724]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6916 - acc: 0.6251    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6915 - acc: 0.6406    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6917 - acc: 0.6144    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6916 - acc: 0.6262    \n",
      "24960/25000 [============================>.] - ETA: 0s10 : [0.6914445723724365, 0.62064]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6909 - acc: 0.6264    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6909 - acc: 0.6458    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6911 - acc: 0.6230    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6910 - acc: 0.6298    \n",
      "24992/25000 [============================>.] - ETA: 0s11 : [0.6908470864486694, 0.6226]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6902 - acc: 0.6315    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6901 - acc: 0.6339    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6904 - acc: 0.6293    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6902 - acc: 0.6410    \n",
      "24960/25000 [============================>.] - ETA: 0s12 : [0.6899591660690307, 0.62704]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6890 - acc: 0.6374    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6888 - acc: 0.6411    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6893 - acc: 0.6333    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6890 - acc: 0.6448    \n",
      "25000/25000 [==============================] - 19s    \n",
      "13 : [0.6885747379684448, 0.63432]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6870 - acc: 0.6558    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6868 - acc: 0.6600    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6873 - acc: 0.6493    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6871 - acc: 0.6584    \n",
      "25000/25000 [==============================] - 18s    \n",
      "14 : [0.6860225966835022, 0.65108]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6818 - acc: 0.6642    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6817 - acc: 0.6826    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6828 - acc: 0.6664    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6821 - acc: 0.6656    \n",
      "25000/25000 [==============================] - 18s    \n",
      "15 : [0.6769770130348206, 0.68216]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6561 - acc: 0.6853    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6531 - acc: 0.6923    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6580 - acc: 0.6790    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6546 - acc: 0.6923    \n",
      "24992/25000 [============================>.] - ETA: 0s16 : [0.6364625122451782, 0.67904]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6418 - acc: 0.6782    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6396 - acc: 0.6794    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6479 - acc: 0.6678    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6371 - acc: 0.6779    \n",
      "25000/25000 [==============================] - 18s    \n",
      "17 : [0.6299882625961304, 0.6788]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6359 - acc: 0.6840    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6355 - acc: 0.6794    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6460 - acc: 0.6757    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6350 - acc: 0.6795    \n",
      "25000/25000 [==============================] - 19s    \n",
      "18 : [0.6241408354187011, 0.68132]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6351 - acc: 0.6818    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6389 - acc: 0.6902    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6402 - acc: 0.6768    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6336 - acc: 0.6906    \n",
      "24992/25000 [============================>.] - ETA: 0s19 : [0.6209150026702881, 0.6876]\n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6323 - acc: 0.6872    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6360 - acc: 0.6944    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 21s - loss: 0.6420 - acc: 0.6843    \n",
      "Epoch 1/1\n",
      "6250/6250 [==============================] - 20s - loss: 0.6273 - acc: 0.6926    \n",
      "25000/25000 [==============================] - 18s    \n",
      "[0.6209150026702881, 0.6876]\n",
      "Baseline Error: 31.24%\n"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "\n",
    "import numpy as np\n",
    "clients = 4\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "\n",
    "X_train_split = np.split(x_train, clients)\n",
    "y_train_split = np.split(y_train, clients)\n",
    "\n",
    "for i in range(clients):\n",
    "  print('Training Model:',i)\n",
    "  model = create_dp_model()\n",
    "  model.fit(X_train_split[i], y_train_split[i], epochs=1, batch_size=32, verbose=1)\n",
    "  model_store.append(model)\n",
    "weights = [model.get_weights() for model in model_store]\n",
    "epochs = 20\n",
    "def avg(weights):\n",
    "  new_weights = []\n",
    "  for weights_list_tuple in zip(*weights):\n",
    "      new_weights.append(\n",
    "          np.array([np.array(weights_).mean(axis=0)\\\n",
    "              for weights_ in zip(*weights_list_tuple)]))\n",
    "  return np.array(new_weights)\n",
    "\n",
    "for j in range(epochs):\n",
    "  weights = [model.get_weights() for model in model_store]\n",
    "  new_weights = avg(weights)\n",
    "  global_model = create_dp_model()\n",
    "  global_model.set_weights(new_weights)\n",
    "  scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "  print(j,':',scores)\n",
    "  for i, model in enumerate(model_store):\n",
    "    model.set_weights(new_weights)\n",
    "    model.fit(X_train_split[i], y_train_split[i], epochs=1, batch_size=32, verbose=1)\n",
    "    #print(model.evaluate(X_test, y_test))\n",
    "\n",
    "global_model = create_dp_model()\n",
    "global_model.set_weights(new_weights)\n",
    "scores = global_model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "\n",
    "#scores = global_dp_model.evaluate(X_test, y_test)\n",
    "print(scores)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_dp_sgd_privacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e34825b63d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_dp_sgd_privacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_dp_sgd_privacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_dp_sgd_privacy' is not defined"
     ]
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=25000, batch_size=32, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_dp_sgd_privacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6e9c84449e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_dp_sgd_privacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_dp_sgd_privacy' is not defined"
     ]
    }
   ],
   "source": [
    "compute_dp_sgd_privacy(n=25000, batch_size=32, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
